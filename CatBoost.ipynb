{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import re\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier, LGBMRanker\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier, Pool\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from surprise import Dataset, Reader, accuracy, SVD, KNNBasic, CoClustering\n",
    "from surprise.dataset import DatasetAutoFolds\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기, 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "def age_map(x: int) -> int:\n",
    "    x = int(x)\n",
    "    if x < 20:\n",
    "        return 1\n",
    "    elif x >= 20 and x < 30:\n",
    "        return 2\n",
    "    elif x >= 30 and x < 40:\n",
    "        return 3\n",
    "    elif x >= 40 and x < 50:\n",
    "        return 4\n",
    "    elif x >= 50 and x < 60:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6\n",
    "\n",
    "def process_context_data(users, books, ratings1, ratings2):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    users : pd.DataFrame\n",
    "        users.csv를 인덱싱한 데이터\n",
    "    books : pd.DataFrame\n",
    "        books.csv를 인덱싱한 데이터\n",
    "    ratings1 : pd.DataFrame\n",
    "        train 데이터의 rating\n",
    "    ratings2 : pd.DataFrame\n",
    "        test 데이터의 rating\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    users['location_city'] = users['location'].apply(lambda x: x.split(',')[0])\n",
    "    users['location_state'] = users['location'].apply(lambda x: x.split(',')[1])\n",
    "    users['location_country'] = users['location'].apply(lambda x: x.split(',')[2])\n",
    "    users = users.drop(['location'], axis=1)\n",
    "\n",
    "    ratings = pd.concat([ratings1, ratings2]).reset_index(drop=True)\n",
    "\n",
    "    # 인덱싱 처리된 데이터 조인\n",
    "    context_df = ratings.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "    train_df = ratings1.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "    test_df = ratings2.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "\n",
    "    # 인덱싱 처리\n",
    "    loc_city2idx = {v:k for k,v in enumerate(context_df['location_city'].unique())}\n",
    "    loc_state2idx = {v:k for k,v in enumerate(context_df['location_state'].unique())}\n",
    "    loc_country2idx = {v:k for k,v in enumerate(context_df['location_country'].unique())}\n",
    "\n",
    "    train_df['location_city'] = train_df['location_city'].map(loc_city2idx)\n",
    "    train_df['location_state'] = train_df['location_state'].map(loc_state2idx)\n",
    "    train_df['location_country'] = train_df['location_country'].map(loc_country2idx)\n",
    "    test_df['location_city'] = test_df['location_city'].map(loc_city2idx)\n",
    "    test_df['location_state'] = test_df['location_state'].map(loc_state2idx)\n",
    "    test_df['location_country'] = test_df['location_country'].map(loc_country2idx)\n",
    "\n",
    "    train_df['age'] = train_df['age'].fillna(int(train_df['age'].mean()))\n",
    "    train_df['age'] = train_df['age'].apply(age_map)\n",
    "    test_df['age'] = test_df['age'].fillna(int(test_df['age'].mean()))\n",
    "    test_df['age'] = test_df['age'].apply(age_map)\n",
    "\n",
    "    # book 파트 인덱싱\n",
    "    category2idx = {v:k for k,v in enumerate(context_df['category'].unique())}\n",
    "    publisher2idx = {v:k for k,v in enumerate(context_df['publisher'].unique())}\n",
    "    language2idx = {v:k for k,v in enumerate(context_df['language'].unique())}\n",
    "    author2idx = {v:k for k,v in enumerate(context_df['book_author'].unique())}\n",
    "\n",
    "    train_df['category'] = train_df['category'].map(category2idx)\n",
    "    train_df['publisher'] = train_df['publisher'].map(publisher2idx)\n",
    "    train_df['language'] = train_df['language'].map(language2idx)\n",
    "    train_df['book_author'] = train_df['book_author'].map(author2idx)\n",
    "    test_df['category'] = test_df['category'].map(category2idx)\n",
    "    test_df['publisher'] = test_df['publisher'].map(publisher2idx)\n",
    "    test_df['language'] = test_df['language'].map(language2idx)\n",
    "    test_df['book_author'] = test_df['book_author'].map(author2idx)\n",
    "\n",
    "    idx = {\n",
    "        \"loc_city2idx\":loc_city2idx,\n",
    "        \"loc_state2idx\":loc_state2idx,\n",
    "        \"loc_country2idx\":loc_country2idx,\n",
    "        \"category2idx\":category2idx,\n",
    "        \"publisher2idx\":publisher2idx,\n",
    "        \"language2idx\":language2idx,\n",
    "        \"author2idx\":author2idx,\n",
    "    }\n",
    "\n",
    "    return idx, train_df, test_df\n",
    "\n",
    "users = pd.read_csv('./data/' + 'users.csv')\n",
    "books = pd.read_csv('./data/' + 'books.csv')\n",
    "train = pd.read_csv('./data/' + 'train_ratings.csv')\n",
    "test = pd.read_csv('./data/' + 'test_ratings.csv')\n",
    "sub = pd.read_csv('./data/' + 'sample_submission.csv')\n",
    "\n",
    "ids = pd.concat([train['user_id'], sub['user_id']]).unique()\n",
    "isbns = pd.concat([train['isbn'], sub['isbn']]).unique()\n",
    "\n",
    "idx2user = {idx:id for idx, id in enumerate(ids)}\n",
    "idx2isbn = {idx:isbn for idx, isbn in enumerate(isbns)}\n",
    "\n",
    "user2idx = {id:idx for idx, id in idx2user.items()}\n",
    "isbn2idx = {isbn:idx for idx, isbn in idx2isbn.items()}\n",
    "\n",
    "train['user_id'] = train['user_id'].map(user2idx)\n",
    "sub['user_id'] = sub['user_id'].map(user2idx)\n",
    "test['user_id'] = test['user_id'].map(user2idx)\n",
    "users['user_id'] = users['user_id'].map(user2idx)\n",
    "\n",
    "train['isbn'] = train['isbn'].map(isbn2idx)\n",
    "sub['isbn'] = sub['isbn'].map(isbn2idx)\n",
    "test['isbn'] = test['isbn'].map(isbn2idx)\n",
    "books['isbn'] = books['isbn'].map(isbn2idx)\n",
    "\n",
    "idx, context_train, context_test = process_context_data(users, books, train, test)\n",
    "field_dims = np.array([len(user2idx), len(isbn2idx),\n",
    "                6, len(idx['loc_city2idx']), len(idx['loc_state2idx']), len(idx['loc_country2idx']),\n",
    "                len(idx['category2idx']), len(idx['publisher2idx']), len(idx['language2idx']), len(idx['author2idx'])], dtype=np.uint32)\n",
    "\n",
    "data = {\n",
    "'train':context_train,\n",
    "'test':context_test.drop(['rating'], axis=1),\n",
    "'field_dims':field_dims,\n",
    "'users':users,\n",
    "'books':books,\n",
    "'sub':sub,\n",
    "'idx2user':idx2user,\n",
    "'idx2isbn':idx2isbn,\n",
    "'user2idx':user2idx,\n",
    "'isbn2idx':isbn2idx,\n",
    "}\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                                            data['train'].drop(['rating'], axis=1),\n",
    "                                            data['train']['rating'],\n",
    "                                            test_size=0.2,\n",
    "                                            random_state=42,\n",
    "                                            shuffle=True\n",
    "                                            )\n",
    "data['X_train'], data['X_valid'], data['y_train'], data['y_valid'] = X_train, X_valid, y_train, y_valid\n",
    "\n",
    "train_dataset = TensorDataset(torch.LongTensor(data['X_train'].values), torch.LongTensor(data['y_train'].values))\n",
    "valid_dataset = TensorDataset(torch.LongTensor(data['X_valid'].values), torch.LongTensor(data['y_valid'].values))\n",
    "test_dataset = TensorDataset(torch.LongTensor(data['test'].values))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1024, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "data['train_dataloader'], data['valid_dataloader'], data['test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X_train'] = data['X_train'].drop(['location_city', 'location_state'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoostClassifier 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "param = {}\n",
    "params = {}\n",
    "params['iterations'] = 100\n",
    "params['learning_rate']=0.1\n",
    "params['depth']=8\n",
    "param['catboost'] = params\n",
    "catboost_cl = CatBoostClassifier(**params, verbose=True, random_state=42)\n",
    "\n",
    "catboost_cl.fit(data['X_train'].select_dtypes(exclude='object'), y_train, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = catboost_cl.predict(data['test'].select_dtypes(exclude='object'))\n",
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission['rating'] = pred.squeeze(1)\n",
    "submission.to_csv('./code/submit/first.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoostRegressor 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_r = CatBoostRegressor(**params,od_pval=0, l2_leaf_reg=0, verbose=True, random_state=42)\n",
    "catboost_r.fit(data['X_train'], data['y_train'], early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_pred_r = catboost_r.predict(data['test'].select_dtypes(exclude='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission['rating'] = catboost_pred_r\n",
    "submission.to_csv('./code/submit/second.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFold 이용하여 CatBoostRegressor 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "is_holdout = False\n",
    "n_splits = 5\n",
    "iterations = 100\n",
    "patience = 50\n",
    "\n",
    "cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "\n",
    "models = []\n",
    "for tri, vai in cv.split(X_train):\n",
    "    print(\"=\"*50)\n",
    "    preds = []\n",
    "\n",
    "    model = CatBoostRegressor(iterations=iterations,random_state=42,task_type=\"GPU\",eval_metric=\"RMSE\",one_hot_max_size=4)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=patience ,verbose = 100)\n",
    "    \n",
    "    models.append(model)\n",
    "    scores.append(model.get_best_score()[\"validation\"][\"RMSE\"])\n",
    "    if is_holdout:\n",
    "        break    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoostRegressor grid_search 사용해서 하이퍼 파라미터 최적값 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# , l2_leaf_reg=True\n",
    "model = CatBoostRegressor(verbose=True, random_state=42)\n",
    "params = {'iterations':[50, 100, 200], 'learning_rate':[0.1, 0.05, 0.01], 'depth':[6, 8, 10]}\n",
    "grid_search_result = model.grid_search(params, X=data['X_train'], y=data['y_train'])\n",
    "# catboost_r.fit(data['X_train'], data['y_train'], early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(data['test'].select_dtypes(exclude='object'))\n",
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission['rating'] = pred\n",
    "submission.to_csv('./code/submit/second.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.6625509 , 7.86972948, 7.59137493, ..., 7.17719288, 6.15119298,\n",
       "       7.01595701])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
